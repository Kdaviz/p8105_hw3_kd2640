Homework 3
================
Keyanna Davis
10/5/2019

Problem 0
=========

This “problem” focuses on structure of your submission, especially the use git and GitHub for reproducibility, R Projects to organize your work, R Markdown to write reproducible reports, relative paths to load data from local files, and reasonable naming structures for your files.

To that end:

-   create a public GitHub repo + local R Project; we suggest naming this repo / directory p8105\_hw3\_YOURUNI (e.g. p8105\_hw3\_ajg2202 for Jeff), but that’s not required
-   create a single .Rmd file named p8105\_hw3\_YOURUNI.Rmd that renders to github\_document

Your solutions to Problems 1, 2, and 3 should be implemented in your .Rmd file, and your git commit history should reflect the process you used to solve these Problems.

For this Problem, we will assess adherence to the instructions above regarding repo structure, git commit history, and whether we are able to knit your .Rmd to ensure that your work is reproducible. Adherence to appropriate styling and clarity of code will be assessed in Problems 1+ using the homework style rubric.

This homework includes figures; the readability of your embedded plots (e.g. font sizes, axis labels, titles) will be assessed in Problems 1+.

Problem 1
=========

The goal is to do some exploration of this dataset. To that end, write a short description of the dataset, noting the size and structure of the data, describing some key variables, and giving illstrative examples of observations. Then, do or answer the following (commenting on the results of each):

``` r
data("instacart") 
instacart
```

    ## # A tibble: 1,384,617 x 15
    ##    order_id product_id add_to_cart_ord… reordered user_id eval_set
    ##       <int>      <int>            <int>     <int>   <int> <chr>   
    ##  1        1      49302                1         1  112108 train   
    ##  2        1      11109                2         1  112108 train   
    ##  3        1      10246                3         0  112108 train   
    ##  4        1      49683                4         0  112108 train   
    ##  5        1      43633                5         1  112108 train   
    ##  6        1      13176                6         0  112108 train   
    ##  7        1      47209                7         0  112108 train   
    ##  8        1      22035                8         1  112108 train   
    ##  9       36      39612                1         0   79431 train   
    ## 10       36      19660                2         1   79431 train   
    ## # … with 1,384,607 more rows, and 9 more variables: order_number <int>,
    ## #   order_dow <int>, order_hour_of_day <int>,
    ## #   days_since_prior_order <int>, product_name <chr>, aisle_id <int>,
    ## #   department_id <int>, aisle <chr>, department <chr>

The instacart dataset has 1384617 rows and 15 columns in the data. All variables are class `numeric` or `character`. Each row contains information about an item that was ordered. The variables include the order information such as order id, reorders, order day of week, etc. The variables also give information about the item such as product name, product id, aisle, department, etc. There are 21 different departments and 39123 different products from these departments in this dataset.

#### Problem 1.1

-   How many aisles are there, and which aisles are the most items ordered from?

``` r
instacart %>% 
  count(aisle) %>% 
   arrange(desc(n)) %>% 
  knitr::kable()
```

| aisle                         |       n|
|:------------------------------|-------:|
| fresh vegetables              |  150609|
| fresh fruits                  |  150473|
| packaged vegetables fruits    |   78493|
| yogurt                        |   55240|
| packaged cheese               |   41699|
| water seltzer sparkling water |   36617|
| milk                          |   32644|
| chips pretzels                |   31269|
| soy lactosefree               |   26240|
| bread                         |   23635|
| refrigerated                  |   23228|
| ice cream ice                 |   22676|
| frozen produce                |   22453|
| eggs                          |   19875|
| crackers                      |   19592|
| frozen meals                  |   18221|
| energy granola bars           |   17449|
| lunch meat                    |   16957|
| soft drinks                   |   16279|
| cereal                        |   16201|
| fresh herbs                   |   16052|
| fresh dips tapenades          |   15142|
| soup broth bouillon           |   15109|
| juice nectars                 |   14350|
| packaged produce              |   13460|
| baby food formula             |   13198|
| baking ingredients            |   13088|
| other creams cheeses          |   12820|
| hot dogs bacon sausage        |   12813|
| paper goods                   |   12694|
| canned jarred vegetables      |   12679|
| nuts seeds dried fruit        |   12532|
| cream                         |   12356|
| spreads                       |   12102|
| canned meals beans            |   11774|
| candy chocolate               |   11453|
| dry pasta                     |   11298|
| oils vinegars                 |   10620|
| butter                        |   10575|
| cookies cakes                 |    9980|
| instant foods                 |    9917|
| breakfast bakery              |    9851|
| condiments                    |    9743|
| pasta sauce                   |    9736|
| frozen breakfast              |    9729|
| tea                           |    9376|
| spices seasonings             |    9279|
| frozen appetizers sides       |    8870|
| coffee                        |    8392|
| tortillas flat bread          |    8353|
| missing                       |    8251|
| frozen pizza                  |    7661|
| asian foods                   |    7007|
| popcorn jerky                 |    6917|
| fruit vegetable snacks        |    6741|
| hot cereal pancake mixes      |    6352|
| grains rice dried goods       |    6134|
| cleaning products             |    5894|
| packaged poultry              |    5608|
| poultry counter               |    5208|
| preserved dips spreads        |    5188|
| tofu meat alternatives        |    5123|
| buns rolls                    |    5054|
| pickled goods olives          |    4882|
| doughs gelatins bake mixes    |    4758|
| energy sports drinks          |    4742|
| frozen vegan vegetarian       |    4727|
| salad dressing toppings       |    4719|
| laundry                       |    4636|
| prepared meals                |    4133|
| canned fruit applesauce       |    3996|
| specialty cheeses             |    3873|
| dish detergents               |    3870|
| granola                       |    3803|
| latino foods                  |    3548|
| frozen meat seafood           |    3341|
| canned meat seafood           |    3241|
| meat counter                  |    3159|
| breakfast bars pastries       |    3144|
| oral hygiene                  |    3070|
| prepared soups salads         |    2936|
| food storage                  |    2906|
| marinades meat preparation    |    2905|
| cat food care                 |    2885|
| honeys syrups nectars         |    2864|
| soap                          |    2773|
| body lotions soap             |    2137|
| vitamins supplements          |    1969|
| plates bowls cups flatware    |    1959|
| beers coolers                 |    1839|
| other                         |    1795|
| refrigerated pudding desserts |    1729|
| fresh pasta                   |    1628|
| trash bags liners             |    1621|
| dog food care                 |    1612|
| protein meal replacements     |    1612|
| frozen breads doughs          |    1532|
| packaged meat                 |    1526|
| bakery desserts               |    1501|
| hair care                     |    1469|
| trail mix snack mix           |    1463|
| cold flu allergy              |    1346|
| red wines                     |    1243|
| digestion                     |    1205|
| diapers wipes                 |    1109|
| baking supplies decor         |    1094|
| white wines                   |    1088|
| seafood counter               |    1084|
| air fresheners candles        |    1067|
| cocoa drink mixes             |    1062|
| feminine care                 |    1048|
| spirits                       |     967|
| mint gum                      |     962|
| frozen dessert                |     922|
| packaged seafood              |     909|
| muscles joints pain relief    |     897|
| more household                |     891|
| deodorants                    |     858|
| facial care                   |     746|
| bulk dried fruits vegetables  |     725|
| indian foods                  |     719|
| bulk grains rice dried goods  |     634|
| kosher foods                  |     628|
| eye ear care                  |     548|
| first aid                     |     539|
| skin care                     |     534|
| shave needs                   |     532|
| ice cream toppings            |     504|
| specialty wines champagnes    |     461|
| kitchen supplies              |     448|
| baby bath body care           |     328|
| baby accessories              |     306|
| frozen juice                  |     294|
| beauty                        |     287|

There are 134 aisle in this dataset. `Fresh vegatables`, `fresh fruits`, and `package vegatables fruits` has the most items ordered from.

#### Problem 1.2

-   Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.

``` r
 instacart %>% 
   group_by(aisle) %>% 
   summarize(n = n()) %>% 
   filter(n > 10000) %>% 
   ggplot(aes( x = n, y = aisle, color = aisle)) +
  geom_point() +
  labs(
    title = "Aisles with more than 10000 Orders",
    x = "total number of items ordered in each aisle",
    y = "aisles"
  ) +
  theme(legend.position = "none")
```

<img src="P8105_hw3_kd2640_files/figure-markdown_github/unnamed-chunk-3-1.png" width="90%" />

According to the plot above the most orders were from `fresh vegatables` and `fresh fruits` about around `150000`. The plot also shows there were alot of aisles with the total amount of items orders ranging from `10000` and `20000`.

#### Problem 1.3

-   Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.

In order for me to make this table I will make a subset from the dataset filtering out the `3` particular aisles separately combine the `3` datasets to make one table.

``` r
BI = instacart %>% 
     filter(aisle%in% c("baking ingredients") ) %>% 
   group_by(aisle, product_name) %>% 
   summarize(n=n()) %>% 
     arrange(desc(n)) %>% 
     head(3)

DC = instacart %>% 
     filter(aisle%in% c("dog food care") ) %>% 
   group_by(aisle, product_name) %>% 
   summarize(n=n()) %>% 
     arrange(desc(n)) %>% 
     head(3)  

PVF = instacart %>% 
     filter(aisle%in% c("packaged vegetables fruits") ) %>% 
   group_by(aisle, product_name) %>% 
   summarize(n=n()) %>% 
     arrange(desc(n)) %>% 
     head(3)  

rbind(BI, DC, PVF) %>%
  arrange(desc(n)) %>% 
  knitr::kable()
```

| aisle                      | product\_name                                 |     n|
|:---------------------------|:----------------------------------------------|-----:|
| packaged vegetables fruits | Organic Baby Spinach                          |  9784|
| packaged vegetables fruits | Organic Raspberries                           |  5546|
| packaged vegetables fruits | Organic Blueberries                           |  4966|
| baking ingredients         | Light Brown Sugar                             |   499|
| baking ingredients         | Pure Baking Soda                              |   387|
| baking ingredients         | Cane Sugar                                    |   336|
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |    30|
| dog food care              | Organix Chicken & Brown Rice Recipe           |    28|
| dog food care              | Small Dog Biscuits                            |    26|

The above table shows that the `3` most popular items from the aisles `packaged vegatables fruits`, `baking ingredients` and `dog food care`. `Organic Baby Spinach` is the most popular item from `packaged vegatables fruits` and the most popular of the `3` aisles. `Pure Baking Soda` is the second most popular item of the `baking ingredients` aisle. `Small Dog Biscuits` is the third most popular item of the `dog food care` aisle.

#### Problem 1.4

-   Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

``` r
apples_and_icecream = instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarise(average_hour = mean(order_hour_of_day)) %>%
  select(product_name, order_dow, average_hour) %>% 
  mutate(
    week_day = recode(
      order_dow,
      '0' = "Sunday",
      '1' = "Monday",
      '2' = "Tuesday",
      '3' = "Wednesday",
      '4' = "Thursday",
      '5' = "Friday",
      '6' = "Saturday"
    )
  ) %>%
  select(product_name, week_day, average_hour) %>% 
  pivot_wider(
    names_from = "week_day",
    values_from = "average_hour") %>% view()
apples_and_icecream %>% 
  knitr::kable()
```

| product\_name    |    Sunday|    Monday|   Tuesday|  Wednesday|  Thursday|    Friday|  Saturday|
|:-----------------|---------:|---------:|---------:|----------:|---------:|---------:|---------:|
| Coffee Ice Cream |  13.77419|  14.31579|  15.38095|   15.31818|  15.21739|  12.26316|  13.83333|
| Pink Lady Apples |  13.44118|  11.36000|  11.70213|   14.25000|  11.55172|  12.78431|  11.93750|

The above table shows the average hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week. On `Sunday` the average hour of the day is about the same for `Coffee Ice Cream` and `Pink Lady Apples` around `13:00`. Also on `Friday` the average hour of the day is about the same for both `Coffee Ice Cream` and `Pink Lady Apples` around `12:00`.

Problem 2
=========

-   do some data cleaning
-   format the data to use appropriate variable names;
-   focus on the “Overall Health” topic
-   include only responses from “Excellent” to “Poor”
-   organize responses as a factor taking levels from “Excellent” to “Poor”

``` r
data("bfrss_smart2010") 

brfss = brfss_smart2010 %>% 
rename("state" = "Locationabbr",
       "county" = "Locationdesc"
       ) %>% 
  mutate(
    county = str_sub(county, 5)
  ) %>% 
     mutate(
     Response = factor(Response, levels = c("Excellent", "Very good", "Good", "Fair", "Poor"))
   ) %>%
   arrange(desc(Response)) %>% 
  filter(Topic=="Overall Health") %>% 
  select(-Question,-(Confidence_limit_Low:GeoLocation)) %>% 
  drop_na()
```

#### Problem 2.1

-   In 2002, which states were observed at 7 or more locations? What about in 2010?

``` r
brfss %>% 
  filter(Year %in% "2002") %>% 
  group_by(state) %>% 
  summarize(n_location = n_distinct(county)) %>% 
  filter(n_location >= 7) %>% 
  knitr::kable()
```

| state |  n\_location|
|:------|------------:|
| CT    |            7|
| FL    |            7|
| MA    |            8|
| NC    |            7|
| PA    |           10|

``` r
brfss %>% 
  filter(Year %in% "2010") %>% 
  group_by(state) %>% 
  summarize(n_location = n_distinct(county)) %>% 
  filter(n_location >= 7) %>%
  knitr::kable(caption = "States with 7 or more Locations")
```

| state |  n\_location|
|:------|------------:|
| CA    |           12|
| CO    |            7|
| FL    |           41|
| MA    |            9|
| MD    |           12|
| NC    |           12|
| NE    |           10|
| NJ    |           19|
| NY    |            9|
| OH    |            8|
| PA    |            7|
| SC    |            7|
| TX    |           16|
| WA    |           10|

In `2002` the states that had 7 or more locations were `Connecticut, FLorida, Massachusetts, North Carolina , New Jersey, and Pennsylvania`. In `2010` the states that had 7 or more locations were `California, Colorado, Florida, Massachusetts, Maryland, North Carolina, Nebraska, New Jersey, New York, Ohio, Pennsylvania, South Carolina, Texas and Washington`.

#### Problem 2.2

-   Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data\_value across locations within a state. Make a “spaghetti” plot of this average value over time within a state (that is, make a plot showing a line for each state across years – the geom\_line geometry and group aesthetic will help)

``` r
brfss %>% 
  filter(Response == "Excellent") %>% 
  select(Response, Year, state, Data_value) %>% 
  group_by(Response, Year, state) %>% 
  summarise(avg_data_value = mean(Data_value)) %>% 
  ggplot(aes(x = Year, y= avg_data_value, group=state, color = state))+
  geom_line() +
   labs(
    title = "Data Values of States Across the Years",
    y = "average data value",
    x = "year"
  )
```

<img src="P8105_hw3_kd2640_files/figure-markdown_github/unnamed-chunk-8-1.png" width="90%" />

The plot shown above indicates that the in `2005` `West Vigrinia` has the lowest `averaged data value` among the rest of the states throughtout the years. In `2003` `Connecticut` has the highest `averaged data value` among the rest of the states throughout the years.

#### Problem 2.3

-   Make a two-panel plot showing, for the years 2006, and 2010, distribution of data\_value for responses (“Poor” to “Excellent”) among locations in NY State.

``` r
brfss %>%
  filter(Year %in% c("2006", "2010"), state == "NY") %>% 
  ggplot(aes(x=Response, y=Data_value, color=Response)) +
  geom_boxplot(alpha = .5) +
  facet_grid(~Year)  + 
  viridis::scale_fill_viridis(discrete = TRUE) +
   labs(
    title = "Distributions of Data Value by Responses of Year 2010 and 2006",
    x = "Response",
    y = "Data Value"
  )
```

<img src="P8105_hw3_kd2640_files/figure-markdown_github/unnamed-chunk-9-1.png" width="90%" />

The above diagram shows a two panel boxplots distributions of data value for responses `Excellent, Very good, Good, Fair, and Poor` for the years `2006` and `2010`. For `Excellent Response` the distribution of data value is skewed to the right for both years `2010` and `2006`. For year `2006` `Good Response` and `Fair Response` distributions of data value are both skewed to the right. In `2006` and `2010` `Very Good Response` distribution data value was skewed to the left. According to the plot above the in `2006` the `Poor Response` the data value was normally distributed but in `2010` it was skewed to the right. In `2010` the `Fair Response` was the data value was normally distributed. Also in `2010` the `Good Response` data value was skewed to the left.

Problem 3
=========

Accelerometers have become an appealing alternative to self-report techniques for studying physical activity in observational studies and clinical trials, largely because of their relative objectivity. During observation periods, the devices measure “activity counts” in a short period; one-minute intervals are common. Because accelerometers can be worn comfortably and unobtrusively, they produce around-the-clock observations.

This problem uses five weeks of accelerometer data collected on a 63 year-old male with BMI 25, who was admitted to the Advanced Cardiac Care Center of Columbia University Medical Center and diagnosed with congestive heart failure (CHF). The data can be downloaded here. **In this spreadsheet, variables `activity.*` are the activity counts for each minute of a 24-hour day starting at midnight.**

\*Load, tidy, and otherwise wrangle the data. Your final dataset should include all originally observed variables and values; have useful variable names; include a weekday vs weekend variable; and encode data with reasonable variable classes. Describe the resulting dataset (e.g. what variables exist, how many observations, etc).

``` r
accel_data = read_csv("~/Desktop/p8105_hw3_kd2640/data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minutes",
    names_prefix = "activity_",
    values_to= "counts"
  )  %>% 
  mutate(
    type_of_day = if_else(day=="Saturday"| day=="Sunday", "weekend", "weekday")
  ) %>% 
  mutate(
   minutes = as.numeric(minutes),
   day = as.factor(day),
   day_id = as.factor(day_id),
   week = as.factor(week),
   counts = as.numeric(counts)
  )
```

    ## Parsed with column specification:
    ## cols(
    ##   .default = col_double(),
    ##   day = col_character()
    ## )

    ## See spec(...) for full column specifications.

The accelerometer dataset has 50400 observations and 6 variables including `week`, `day_id`, `day`, `minutes`, `counts`, and `day_of_week`.

\*Traditional analyses of accelerometer data focus on the total activity over the day. Using your tidied dataset, aggregate accross minutes to create a total activity variable for each day, and create a table showing these totals. Are any trends apparent?

``` r
accel_data2 =
  accel_data %>% 
  select(week, minutes, day, counts, type_of_day) %>% 
  group_by(week, day, type_of_day) %>% 
  summarize(total_activity = sum(counts)) 

  accel_data2 %>% 
  knitr::kable()
```

| week | day       | type\_of\_day |  total\_activity|
|:-----|:----------|:--------------|----------------:|
| 1    | Friday    | weekday       |        480542.62|
| 1    | Monday    | weekday       |         78828.07|
| 1    | Saturday  | weekend       |        376254.00|
| 1    | Sunday    | weekend       |        631105.00|
| 1    | Thursday  | weekday       |        355923.64|
| 1    | Tuesday   | weekday       |        307094.24|
| 1    | Wednesday | weekday       |        340115.01|
| 2    | Friday    | weekday       |        568839.00|
| 2    | Monday    | weekday       |        295431.00|
| 2    | Saturday  | weekend       |        607175.00|
| 2    | Sunday    | weekend       |        422018.00|
| 2    | Thursday  | weekday       |        474048.00|
| 2    | Tuesday   | weekday       |        423245.00|
| 2    | Wednesday | weekday       |        440962.00|
| 3    | Friday    | weekday       |        467420.00|
| 3    | Monday    | weekday       |        685910.00|
| 3    | Saturday  | weekend       |        382928.00|
| 3    | Sunday    | weekend       |        467052.00|
| 3    | Thursday  | weekday       |        371230.00|
| 3    | Tuesday   | weekday       |        381507.00|
| 3    | Wednesday | weekday       |        468869.00|
| 4    | Friday    | weekday       |        154049.00|
| 4    | Monday    | weekday       |        409450.00|
| 4    | Saturday  | weekend       |          1440.00|
| 4    | Sunday    | weekend       |        260617.00|
| 4    | Thursday  | weekday       |        340291.00|
| 4    | Tuesday   | weekday       |        319568.00|
| 4    | Wednesday | weekday       |        434460.00|
| 5    | Friday    | weekday       |        620860.00|
| 5    | Monday    | weekday       |        389080.00|
| 5    | Saturday  | weekend       |          1440.00|
| 5    | Sunday    | weekend       |        138421.00|
| 5    | Thursday  | weekday       |        549658.00|
| 5    | Tuesday   | weekday       |        367824.00|
| 5    | Wednesday | weekday       |        445366.00|

``` r
 accel_data2 %>%  
   ggplot(aes(x= week, y=total_activity, color=day)) +
  geom_point() +
    labs(
    title = "Weekly Total Activity By Days",
    x = "week",
    y = "total activity"
  )
```

<img src="P8105_hw3_kd2640_files/figure-markdown_github/unnamed-chunk-11-1.png" width="90%" />

The above graph shows a `scatter plot`. On `Wednesday` the activity for the 63 year man is steady throughout the weeks. I don't see any trends on the other days of the week it seems the activity is up and down througout the `5` weeks.

#### Problem 3.3

-   Accelerometer data allows the inspection activity over the course of the day. Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week. Describe in words any patterns or conclusions you can make based on this graph.

``` r
accel_data %>% 
  ggplot(aes(x = minutes, y = counts, color = day, group = day_id)) +
  geom_line() +
   labs(
    title = "24-Hour Activity Time",
    x = "time in minutes",
    y = "activity counts"
  )
```

<img src="P8105_hw3_kd2640_files/figure-markdown_github/unnamed-chunk-12-1.png" width="90%" />

From this graph, it seems that the 63 year man is more activity later in the day on `Friday`. During the early part of the day he has lower activity throughtout the days of the week. On `Thursday` he has higher activity in the middle of the day.
